<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>LLM | Ruihao Gong</title>
    <link>https://example.com/tag/llm/</link>
      <atom:link href="https://example.com/tag/llm/index.xml" rel="self" type="application/rss+xml" />
    <description>LLM</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 26 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0218ab8280f4fa11ac3ef903ebdf935e_6403_512x512_fill_lanczos_center_3.png</url>
      <title>LLM</title>
      <link>https://example.com/tag/llm/</link>
    </image>
    
    <item>
      <title>LLMC</title>
      <link>https://example.com/project/llmc/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/llmc/</guid>
      <description>&lt;p&gt;LLMC is an off-the-shell tool designed for compressing LLM, leveraging state-of-the-art compression algorithms to enhance efficiency and reduce model size without compromising performance.&lt;/p&gt;
&lt;h2 id=&#34;highlight-feature&#34;&gt;Highlight Feature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Comprehensive Algorithm Support&lt;/strong&gt;: Provides a broad range of âœ¨&lt;code&gt;SOTA compression algorithms&lt;/code&gt;, including âœ…quantization, âœ…mixed-precision quantization, and âœ…sparsity, while maintaining accuracy consistent with the original repositories. âœ¨&lt;code&gt;Quantization best practices&lt;/code&gt; (see ðŸš€&lt;code&gt;Best Practices&lt;/code&gt; &lt;a href=&#34;https://llmc-en.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) are also available to ensure optimal performance and efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Supported Formats&lt;/strong&gt;: Supports both âœ¨&lt;code&gt;quantization&lt;/code&gt; (integer and floating-point) and âœ¨&lt;code&gt;sparsity&lt;/code&gt;, specifically including âœ…weight-activation, âœ…weight-only, âœ…mixed-precision quantization, as well as âœ…structured and âœ…unstructured sparsity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Wide Model Support&lt;/strong&gt;: Offers support for a diverse array of âœ¨&lt;code&gt;LLM models&lt;/code&gt;, including âœ…LLama, âœ…Mistral, âœ…InternLM2, âœ…Qwen2, among others, as well as âœ…MOE(DeepSeekv2, Deepseekv2.5) and âœ…VLM(Llama3.2-vision, Qwen2-vl) models (see &lt;a href=&#34;#supported-model-list&#34;&gt;Supported Model List&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Multi-backend Compatibility&lt;/strong&gt;: Seamlessly integrates with various backends for enhanced deployment flexibility. Multiple quantization settings and model formats are compatible with a wide range of backends and hardware platforms, such as âœ…VLLM, âœ…Sglang, âœ…LightLLM, âœ…MLC-LLM, and âœ…AutoAWQ, making it highly versatile(see Section &lt;code&gt;Backend&lt;/code&gt; &lt;a href=&#34;https://llmc-en.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Performance Efficiency&lt;/strong&gt;: Enables quantization of large LLMs, such as âœ¨&lt;code&gt;Llama3.1-405B&lt;/code&gt; and âœ¨&lt;code&gt;DeepSeekV2-236B&lt;/code&gt;, with PPL evaluation on a &lt;code&gt;single A100/H100/H800 GPU&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>LightLLM</title>
      <link>https://example.com/project/lightllm/</link>
      <pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/lightllm/</guid>
      <description>&lt;p&gt;LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance. LightLLM harnesses the strengths of numerous well-regarded open-source implementations, including but not limited to FasterTransformer, TGI, vLLM, and FlashAttention.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tri-process asynchronous collaboration: tokenization, model inference, and detokenization are performed asynchronously, leading to a considerable improvement in GPU utilization.&lt;/li&gt;
&lt;li&gt;Nopad (Unpad): offers support for nopad attention operations across multiple models to efficiently handle requests with large length disparities.&lt;/li&gt;
&lt;li&gt;Dynamic Batch: enables dynamic batch scheduling of requests&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FlashAttention&lt;/a&gt;: incorporates FlashAttention to improve speed and reduce GPU memory footprint during inference.&lt;/li&gt;
&lt;li&gt;Tensor Parallelism: utilizes tensor parallelism over multiple GPUs for faster inference.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./docs/TokenAttention.md&#34;&gt;Token Attention&lt;/a&gt;: implements token-wise&amp;rsquo;s KV cache memory management mechanism, allowing for zero memory waste during inference.&lt;/li&gt;
&lt;li&gt;High-performance Router: collaborates with Token Attention to meticulously manage the GPU memory of each token, thereby optimizing system throughput.&lt;/li&gt;
&lt;li&gt;Int8KV Cache: This feature will increase the capacity of tokens to almost twice as much. only llama support.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;supported-model-list&#34;&gt;Supported Model List&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BLOOM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLaMA V2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StarCoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-7B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGLM2-6b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-7B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan2-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan2-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternLM-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/01-ai/Yi-34B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yi-34b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-VL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL-Chat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-VL-Chat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llava-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-13b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llava-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Mixtral&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-2-1_6b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stablelm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniCPM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>EasyLLM</title>
      <link>https://example.com/project/easyllm/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/easyllm/</guid>
      <description>&lt;p&gt;Built upon Megatron-Deepspeed and HuggingFace Trainer, EasyLLM has reorganized the code logic with a focus on usability. While enhancing usability, it also ensures training efficiency.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
