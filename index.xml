<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Ruihao Gong</title>
    <link>https://example.com/</link>
      <atom:link href="https://example.com/index.xml" rel="self" type="application/rss+xml" />
    <description>Ruihao Gong</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Tue, 26 Nov 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu0218ab8280f4fa11ac3ef903ebdf935e_6403_512x512_fill_lanczos_center_3.png</url>
      <title>Ruihao Gong</title>
      <link>https://example.com/</link>
    </image>
    
    <item>
      <title>LLMC</title>
      <link>https://example.com/project/llmc/</link>
      <pubDate>Tue, 26 Nov 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/llmc/</guid>
      <description>&lt;p&gt;LLMC is an off-the-shell tool designed for compressing LLM, leveraging state-of-the-art compression algorithms to enhance efficiency and reduce model size without compromising performance.&lt;/p&gt;
&lt;h2 id=&#34;highlight-feature&#34;&gt;Highlight Feature&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Comprehensive Algorithm Support&lt;/strong&gt;: Provides a broad range of âœ¨&lt;code&gt;SOTA compression algorithms&lt;/code&gt;, including âœ…quantization, âœ…mixed-precision quantization, and âœ…sparsity, while maintaining accuracy consistent with the original repositories. âœ¨&lt;code&gt;Quantization best practices&lt;/code&gt; (see ðŸš€&lt;code&gt;Best Practices&lt;/code&gt; &lt;a href=&#34;https://llmc-en.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;) are also available to ensure optimal performance and efficiency.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Supported Formats&lt;/strong&gt;: Supports both âœ¨&lt;code&gt;quantization&lt;/code&gt; (integer and floating-point) and âœ¨&lt;code&gt;sparsity&lt;/code&gt;, specifically including âœ…weight-activation, âœ…weight-only, âœ…mixed-precision quantization, as well as âœ…structured and âœ…unstructured sparsity.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Wide Model Support&lt;/strong&gt;: Offers support for a diverse array of âœ¨&lt;code&gt;LLM models&lt;/code&gt;, including âœ…LLama, âœ…Mistral, âœ…InternLM2, âœ…Qwen2, among others, as well as âœ…MOE(DeepSeekv2, Deepseekv2.5) and âœ…VLM(Llama3.2-vision, Qwen2-vl) models (see &lt;a href=&#34;#supported-model-list&#34;&gt;Supported Model List&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Multi-backend Compatibility&lt;/strong&gt;: Seamlessly integrates with various backends for enhanced deployment flexibility. Multiple quantization settings and model formats are compatible with a wide range of backends and hardware platforms, such as âœ…VLLM, âœ…Sglang, âœ…LightLLM, âœ…MLC-LLM, and âœ…AutoAWQ, making it highly versatile(see Section &lt;code&gt;Backend&lt;/code&gt; &lt;a href=&#34;https://llmc-en.readthedocs.io/en/latest/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;ðŸ’¥&lt;strong&gt;Performance Efficiency&lt;/strong&gt;: Enables quantization of large LLMs, such as âœ¨&lt;code&gt;Llama3.1-405B&lt;/code&gt; and âœ¨&lt;code&gt;DeepSeekV2-236B&lt;/code&gt;, with PPL evaluation on a &lt;code&gt;single A100/H100/H800 GPU&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes</title>
      <link>https://example.com/publication/2024-aaai-pts/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2024-aaai-pts/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Selective Focus: Investigating Semantics Sensitivity in Post-training Quantization for Lane Detection</title>
      <link>https://example.com/publication/2024-aaai-lanequant/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2024-aaai-lanequant/</guid>
      <description></description>
    </item>
    
    <item>
      <title>TFMQ-DM: Temporal Feature Maintenance Quantization for Diffusion Models</title>
      <link>https://example.com/publication/2024-cvpr-tfmq-dm/</link>
      <pubDate>Sat, 01 Jun 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2024-cvpr-tfmq-dm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LightLLM</title>
      <link>https://example.com/project/lightllm/</link>
      <pubDate>Sat, 27 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/lightllm/</guid>
      <description>&lt;p&gt;LightLLM is a Python-based LLM (Large Language Model) inference and serving framework, notable for its lightweight design, easy scalability, and high-speed performance. LightLLM harnesses the strengths of numerous well-regarded open-source implementations, including but not limited to FasterTransformer, TGI, vLLM, and FlashAttention.&lt;/p&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Tri-process asynchronous collaboration: tokenization, model inference, and detokenization are performed asynchronously, leading to a considerable improvement in GPU utilization.&lt;/li&gt;
&lt;li&gt;Nopad (Unpad): offers support for nopad attention operations across multiple models to efficiently handle requests with large length disparities.&lt;/li&gt;
&lt;li&gt;Dynamic Batch: enables dynamic batch scheduling of requests&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/Dao-AILab/flash-attention&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;FlashAttention&lt;/a&gt;: incorporates FlashAttention to improve speed and reduce GPU memory footprint during inference.&lt;/li&gt;
&lt;li&gt;Tensor Parallelism: utilizes tensor parallelism over multiple GPUs for faster inference.&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;./docs/TokenAttention.md&#34;&gt;Token Attention&lt;/a&gt;: implements token-wise&amp;rsquo;s KV cache memory management mechanism, allowing for zero memory waste during inference.&lt;/li&gt;
&lt;li&gt;High-performance Router: collaborates with Token Attention to meticulously manage the GPU memory of each token, thereby optimizing system throughput.&lt;/li&gt;
&lt;li&gt;Int8KV Cache: This feature will increase the capacity of tokens to almost twice as much. only llama support.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;supported-model-list&#34;&gt;Supported Model List&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/bigscience/bloom&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BLOOM&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/facebookresearch/llama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLaMA&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/meta-llama&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LLaMA V2&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/bigcode-project/starcoder&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;StarCoder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/QwenLM/Qwen-7B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/THUDM/ChatGLM2-6B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ChatGLM2-6b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-7B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan2-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan2-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/baichuan-inc/Baichuan-13B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Baichuan-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/InternLM/InternLM&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;InternLM-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/01-ai/Yi-34B&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Yi-34b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-VL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/Qwen/Qwen-VL-Chat&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qwen-VL-Chat&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-7b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llava-7b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/liuhaotian/llava-v1.5-13b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Llava-13b&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;&#34;&gt;Mixtral&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/stabilityai/stablelm-2-1_6b&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stablelm&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/openbmb/MiniCPM-2B-sft-bf16&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MiniCPM&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>EasyLLM</title>
      <link>https://example.com/project/easyllm/</link>
      <pubDate>Fri, 26 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/easyllm/</guid>
      <description>&lt;p&gt;Built upon Megatron-Deepspeed and HuggingFace Trainer, EasyLLM has reorganized the code logic with a focus on usability. While enhancing usability, it also ensures training efficiency.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models</title>
      <link>https://example.com/publication/2024-iclr-qllm/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2024-iclr-qllm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rectify representation bias in vision-language models for long-tailed recognition</title>
      <link>https://example.com/publication/2023-nn-rect/</link>
      <pubDate>Mon, 01 Jan 2024 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-nn-rect/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outlier Suppression&#43;: Accurate quantization of large language models by equivalent and effective shifting and scaling</title>
      <link>https://example.com/publication/2023-emnlp-osplus/</link>
      <pubDate>Fri, 01 Dec 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-emnlp-osplus/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Lossy and Lossless (L2) Post-training Model Size Compression</title>
      <link>https://example.com/publication/2023-iccv-l-2-compress/</link>
      <pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-iccv-l-2-compress/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LPCV 2023 Winner Solution</title>
      <link>https://example.com/project/lpcv2023-fpga-track/</link>
      <pubDate>Sun, 27 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/lpcv2023-fpga-track/</guid>
      <description>&lt;p&gt;This project contains the complete implementation that wins the LPCV 2023 Challenge.&lt;/p&gt;
&lt;p&gt;The leaderboard can be seen at &lt;a href=&#34;https://lpcv.ai/scoreboard/Segmentation23&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LPCV2023 Leaderboard&lt;/a&gt; (ModelTC Team).&lt;/p&gt;
&lt;p&gt;Code:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ModelTC/LPCV_2023_solution&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ModelTC/LPCV_2023_solution&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/ModelTC/UP_LPCV2023_Plugin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/ModelTC/UP_LPCV2023_Plugin&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Annealing-Based Label-Transfer Learning for Open World Object Detection</title>
      <link>https://example.com/publication/2023-cvpr-allow/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-cvpr-allow/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploring the Relationship Between Architectural Design and Adversarially Robust Generalization</title>
      <link>https://example.com/publication/2023-cvpr-robust-model/</link>
      <pubDate>Thu, 01 Jun 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-cvpr-robust-model/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Discrepant Semantic Diffusion Boosts Transfer Learning Robustness</title>
      <link>https://example.com/publication/2023-electronics-semantic-diffusion/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-electronics-semantic-diffusion/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Exploiting Subgraph Similarities for Efficient Auto-tuning of Tensor Programs</title>
      <link>https://example.com/publication/2023-icpp-familyseer/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-icpp-familyseer/</guid>
      <description></description>
    </item>
    
    <item>
      <title>SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency</title>
      <link>https://example.com/publication/2023-mlsys-sysnoise/</link>
      <pubDate>Sun, 01 Jan 2023 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2023-mlsys-sysnoise/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Distribution-Sensitive Information Retention for Accurate Binary Neural Network</title>
      <link>https://example.com/publication/2022-ijcv-bnn/</link>
      <pubDate>Sat, 01 Oct 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2022-ijcv-bnn/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Outlier Suppression: Pushing the Limit of Low-bit Transformer Language Models</title>
      <link>https://example.com/publication/2022-neurips-outlier-suppression/</link>
      <pubDate>Tue, 27 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2022-neurips-outlier-suppression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>When industrial model toolchain meets Xilinx FPGA</title>
      <link>https://example.com/talk/when-industrial-model-toolchain-meets-xilinx-fpga/</link>
      <pubDate>Sat, 21 May 2022 05:30:00 +0000</pubDate>
      <guid>https://example.com/talk/when-industrial-model-toolchain-meets-xilinx-fpga/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Generating Transferable Adversarial Examples against Vision Transformers</title>
      <link>https://example.com/publication/2022-mm-adv-vit/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2022-mm-adv-vit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>NNLQP: A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database</title>
      <link>https://example.com/publication/2022-icpp-nnlqp/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2022-icpp-nnlqp/</guid>
      <description></description>
    </item>
    
    <item>
      <title>QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization</title>
      <link>https://example.com/publication/2022-iclr-qdrop/</link>
      <pubDate>Sat, 01 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2022-iclr-qdrop/</guid>
      <description></description>
    </item>
    
    <item>
      <title>MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing</title>
      <link>https://example.com/publication/2021-iccv-mixmix/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-iccv-mixmix/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Once Quantization-Aware Training: High Performance Extremely Low-Bit Architecture Search</title>
      <link>https://example.com/publication/2021-iccv-oqa/</link>
      <pubDate>Fri, 01 Oct 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-iccv-oqa/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Research on Post-training Quantization - From classic to original</title>
      <link>https://example.com/talk/research-on-post-training-quantization-from-classic-to-original/</link>
      <pubDate>Fri, 27 Aug 2021 19:00:00 +0000</pubDate>
      <guid>https://example.com/talk/research-on-post-training-quantization-from-classic-to-original/</guid>
      <description>&lt;p&gt;A summary can be seen at &lt;a href=&#34;https://mp.weixin.qq.com/s/cuzBmjw_4cdmLlgVHHS03Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>LPCV 2021 Winner Solution of FPGA Track</title>
      <link>https://example.com/project/lpcv2021-fpga-track/</link>
      <pubDate>Fri, 27 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/lpcv2021-fpga-track/</guid>
      <description>&lt;p&gt;This project contains the complete implementation that wins the LPCV 2021 FPGA track, which can run an object detection model with an extremely high efficiency and accuracy.&lt;/p&gt;
&lt;p&gt;The leaderboard can be seen at &lt;a href=&#34;https://lpcv.ai/scoreboard/FPGA21&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;LPCV2021 Leaderboard&lt;/a&gt; (Spring Team).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Block Sparsity on TVM -- 50% spasity = 1.6 speed up &#43; &lt; 1% accuracy loss</title>
      <link>https://example.com/post/2021-tvm-sparsity/</link>
      <pubDate>Thu, 26 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2021-tvm-sparsity/</guid>
      <description>&lt;p&gt;For details, please refer to &lt;a href=&#34;https://zhuanlan.zhihu.com/p/403869592&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quantization frameworks</title>
      <link>https://example.com/post/2021-quant-tool/</link>
      <pubDate>Tue, 17 Aug 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2021-quant-tool/</guid>
      <description>&lt;p&gt;For details, please refer to &lt;a href=&#34;https://zhuanlan.zhihu.com/p/355598250&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MQBench</title>
      <link>https://example.com/project/mqbench/</link>
      <pubDate>Tue, 27 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/mqbench/</guid>
      <description>&lt;p&gt;MQBench is an open-source model quantization toolkit based on PyTorch fx.&lt;/p&gt;
&lt;p&gt;The envision of MQBench is to provide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SOTA Algorithms. With MQBench, the hardware vendors and researchers can benefit from the latest research progress in academia.&lt;/li&gt;
&lt;li&gt;Powerful Toolkits. With the toolkit, quantization node can be inserted to the original PyTorch module automatically with respect to the specific hardware. After training, the quantized model can be smoothly converted to the format that can inference on the real device.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The documentation can be seen at &lt;a href=&#34;https://mqbench.readthedocs.io/en/latest/?badge=latest&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt; and the website is at &lt;a href=&#34;http://mqbench.tech/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MQBench: Towards Reproducible and Deployable Model Quantization Benchmark</title>
      <link>https://example.com/publication/2021-neuips-mqbench/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-neuips-mqbench/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RobustART</title>
      <link>https://example.com/project/robustart/</link>
      <pubDate>Sun, 27 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/project/robustart/</guid>
      <description>&lt;p&gt;RobustART is a comprehensive Robustness investigation benchmark on ImageNet regarding ARchitectural design (49 human-designed off-the-shelf architectures and neural architecture searched networks) and Training techniques (10+ general ones e.g., extra training data, etc) towards diverse noises (adversarial, natural, and system noises).&lt;/p&gt;
&lt;p&gt;The benchmark (including open-source toolkit, pre-trained model zoo, datasets, and analyses):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;presents an open-source platform for conducting comprehensive evaluation on diverse robustness types;&lt;/li&gt;
&lt;li&gt;provides a variety of pre-trained models with different training techniques to facilitate robustness evaluation;&lt;/li&gt;
&lt;li&gt;proposes a new view to better understand the mechanism towards designing robust DNN architectures, backed up by the analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For more details, please refer to the &lt;a href=&#34;http://robust.art/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;. We will continuously contribute to building this ecosystem for the community.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Diversifying Sample Generation for Accurate Data-Free Quantization</title>
      <link>https://example.com/publication/2021-cvpr-dsg/</link>
      <pubDate>Tue, 01 Jun 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-cvpr-dsg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Neural Network Model Compression</title>
      <link>https://example.com/talk/neural-network-model-compression/</link>
      <pubDate>Thu, 06 May 2021 19:00:00 +0000</pubDate>
      <guid>https://example.com/talk/neural-network-model-compression/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Int8 ViT on TVM, 1.5 speed up compared with TensorRT</title>
      <link>https://example.com/post/2021-vit-tvm-quant/</link>
      <pubDate>Mon, 19 Apr 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2021-vit-tvm-quant/</guid>
      <description>&lt;p&gt;For details, please refer to &lt;a href=&#34;https://mp.weixin.qq.com/s/BtxmGr2YUyY1zDdL4sCJ9w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>An Introduction to Block Reconstruction Quantization</title>
      <link>https://example.com/talk/an-introduction-to-block-reconstruction-quantization/</link>
      <pubDate>Wed, 14 Apr 2021 19:00:00 +0000</pubDate>
      <guid>https://example.com/talk/an-introduction-to-block-reconstruction-quantization/</guid>
      <description>&lt;p&gt;A summary can be seen at &lt;a href=&#34;https://mp.weixin.qq.com/s/KDez283UKmnY6Ry3K1d7-g&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Gap bettween the academia and industry, from the quantiztion perspective</title>
      <link>https://example.com/post/2021-quant-gap/</link>
      <pubDate>Mon, 08 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/post/2021-quant-gap/</guid>
      <description>&lt;p&gt;For details, please refer to &lt;a href=&#34;https://zhuanlan.zhihu.com/p/349820682&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction</title>
      <link>https://example.com/publication/2021-iclr-brecq/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-iclr-brecq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>RobustART: Benchmarking Robustness on Architecture Design and Training Techniques</title>
      <link>https://example.com/publication/2021-arxiv-robustart/</link>
      <pubDate>Fri, 01 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2021-arxiv-robustart/</guid>
      <description></description>
    </item>
    
    <item>
      <title>An Introduction to Quantization</title>
      <link>https://example.com/talk/an-introduction-to-quantization/</link>
      <pubDate>Tue, 09 Jun 2020 19:00:00 +0000</pubDate>
      <guid>https://example.com/talk/an-introduction-to-quantization/</guid>
      <description>&lt;p&gt;A summary can be seen at &lt;a href=&#34;https://mp.weixin.qq.com/s/yC2Jb4feobD1MttblHw_xg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Link&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Forward and Backward Information Retention for Accurate Binary Neural Networks</title>
      <link>https://example.com/publication/2020-cvpr-irnet/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-cvpr-irnet/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Rotation Consistent Margin Loss for Efficient Low-bit Face Recognition</title>
      <link>https://example.com/publication/2020-cvpr-rcmloss/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-cvpr-rcmloss/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Towards Unified INT8 Training for Convolutional Neural Network</title>
      <link>https://example.com/publication/2020-cvpr-int-8-training/</link>
      <pubDate>Mon, 01 Jun 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-cvpr-int-8-training/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Balanced Binary Neural Networks with Gated Residual</title>
      <link>https://example.com/publication/2020-icassp-bbg/</link>
      <pubDate>Fri, 01 May 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-icassp-bbg/</guid>
      <description></description>
    </item>
    
    <item>
      <title>DMS: Differentiable Dimension Search for Binary Neural Networks</title>
      <link>https://example.com/publication/2020-iclrw-dms/</link>
      <pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-iclrw-dms/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Binary neural networks: A survey</title>
      <link>https://example.com/publication/2020-pr-bnnsurvey/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-pr-bnnsurvey/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Efficient Bitwidth Search for Practical Mixed Precision Neural Network</title>
      <link>https://example.com/publication/2020-arxiv-ebs/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-arxiv-ebs/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Extremely Low-Bit Convolution Optimization for Quantized Neural Network on Modern Computer Architectures</title>
      <link>https://example.com/publication/2020-icpp-lowbit/</link>
      <pubDate>Wed, 01 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2020-icpp-lowbit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks</title>
      <link>https://example.com/publication/2019-iccv-dsq/</link>
      <pubDate>Tue, 01 Oct 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/publication/2019-iccv-dsq/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Slides</title>
      <link>https://example.com/slides/example/</link>
      <pubDate>Tue, 05 Feb 2019 00:00:00 +0000</pubDate>
      <guid>https://example.com/slides/example/</guid>
      <description>&lt;h1 id=&#34;create-slides-in-markdown-with-wowchemy&#34;&gt;Create slides in Markdown with Wowchemy&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Wowchemy&lt;/a&gt; | &lt;a href=&#34;https://owchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;features&#34;&gt;Features&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Efficiently write slides in Markdown&lt;/li&gt;
&lt;li&gt;3-in-1: Create, Present, and Publish your slides&lt;/li&gt;
&lt;li&gt;Supports speaker notes&lt;/li&gt;
&lt;li&gt;Mobile friendly slides&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;controls&#34;&gt;Controls&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Next: &lt;code&gt;Right Arrow&lt;/code&gt; or &lt;code&gt;Space&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Previous: &lt;code&gt;Left Arrow&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Start: &lt;code&gt;Home&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Finish: &lt;code&gt;End&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Overview: &lt;code&gt;Esc&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Speaker notes: &lt;code&gt;S&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Fullscreen: &lt;code&gt;F&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Zoom: &lt;code&gt;Alt + Click&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/hakimel/reveal.js#pdf-export&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;PDF Export&lt;/a&gt;: &lt;code&gt;E&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;code-highlighting&#34;&gt;Code Highlighting&lt;/h2&gt;
&lt;p&gt;Inline code: &lt;code&gt;variable&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Code block:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;porridge = &amp;quot;blueberry&amp;quot;
if porridge == &amp;quot;blueberry&amp;quot;:
    print(&amp;quot;Eating...&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;math&#34;&gt;Math&lt;/h2&gt;
&lt;p&gt;In-line math: $x + y = z$&lt;/p&gt;
&lt;p&gt;Block math:&lt;/p&gt;
&lt;p&gt;$$
f\left( x \right) = ;\frac{{2\left( {x + 4} \right)\left( {x - 4} \right)}}{{\left( {x + 4} \right)\left( {x + 1} \right)}}
$$&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;fragments&#34;&gt;Fragments&lt;/h2&gt;
&lt;p&gt;Make content appear incrementally&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{{% fragment %}} One {{% /fragment %}}
{{% fragment %}} **Two** {{% /fragment %}}
{{% fragment %}} Three {{% /fragment %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press &lt;code&gt;Space&lt;/code&gt; to play!&lt;/p&gt;
&lt;span class=&#34;fragment &#34; &gt;
  One
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  &lt;strong&gt;Two&lt;/strong&gt;
&lt;/span&gt;
&lt;span class=&#34;fragment &#34; &gt;
  Three
&lt;/span&gt;
&lt;hr&gt;
&lt;p&gt;A fragment can accept two optional parameters:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;class&lt;/code&gt;: use a custom style (requires definition in custom CSS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;weight&lt;/code&gt;: sets the order in which a fragment appears&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;h2 id=&#34;speaker-notes&#34;&gt;Speaker Notes&lt;/h2&gt;
&lt;p&gt;Add speaker notes to your presentation&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{% speaker_note %}}
- Only the speaker can read these notes
- Press `S` key to view
{{% /speaker_note %}}
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Press the &lt;code&gt;S&lt;/code&gt; key to view the speaker notes!&lt;/p&gt;
&lt;aside class=&#34;notes&#34;&gt;
  &lt;ul&gt;
&lt;li&gt;Only the speaker can read these notes&lt;/li&gt;
&lt;li&gt;Press &lt;code&gt;S&lt;/code&gt; key to view&lt;/li&gt;
&lt;/ul&gt;

&lt;/aside&gt;
&lt;hr&gt;
&lt;h2 id=&#34;themes&#34;&gt;Themes&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;black: Black background, white text, blue links (default)&lt;/li&gt;
&lt;li&gt;white: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;league: Gray background, white text, blue links&lt;/li&gt;
&lt;li&gt;beige: Beige background, dark text, brown links&lt;/li&gt;
&lt;li&gt;sky: Blue background, thin dark text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;ul&gt;
&lt;li&gt;night: Black background, thick white text, orange links&lt;/li&gt;
&lt;li&gt;serif: Cappuccino background, gray text, brown links&lt;/li&gt;
&lt;li&gt;simple: White background, black text, blue links&lt;/li&gt;
&lt;li&gt;solarized: Cream-colored background, dark green text, blue links&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;

&lt;section data-noprocess data-shortcode-slide
  
      
      data-background-image=&#34;/media/boards.jpg&#34;
  &gt;

&lt;h2 id=&#34;custom-slide&#34;&gt;Custom Slide&lt;/h2&gt;
&lt;p&gt;Customize the slide style and background&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-markdown&#34;&gt;{{&amp;lt; slide background-image=&amp;quot;/media/boards.jpg&amp;quot; &amp;gt;}}
{{&amp;lt; slide background-color=&amp;quot;#0000FF&amp;quot; &amp;gt;}}
{{&amp;lt; slide class=&amp;quot;my-style&amp;quot; &amp;gt;}}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h2 id=&#34;custom-css-example&#34;&gt;Custom CSS Example&lt;/h2&gt;
&lt;p&gt;Let&amp;rsquo;s make headers navy colored.&lt;/p&gt;
&lt;p&gt;Create &lt;code&gt;assets/css/reveal_custom.css&lt;/code&gt; with:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&#34;language-css&#34;&gt;.reveal section h1,
.reveal section h2,
.reveal section h3 {
  color: navy;
}
&lt;/code&gt;&lt;/pre&gt;
&lt;hr&gt;
&lt;h1 id=&#34;questions&#34;&gt;Questions?&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/wowchemy/wowchemy-hugo-modules/discussions&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Ask&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Documentation&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title></title>
      <link>https://example.com/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://example.com/admin/config.yml</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
