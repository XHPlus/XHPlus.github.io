[{"authors":null,"categories":null,"content":"Ruihao Gong is currently a senior research manager and team leader for Efficient Model R\u0026amp;D at SenseTime Research Model Toolchain Team under the supervision of Xiaogang Wang and Fengwei Yu. He focuses on the system and algorithms for accelerating the industry model production, model deployment and model efficiency. His research interests include model quantization, model sparsity, hardware-friendly neural networks for various hardwares such as cloud servers and mobile/egde devices, and various applications such as smart city and L2/L4 autonomous driving.\nNews:\n[2022.6] Our Neural Network Latency Query and Prediction (NNLQP) system is accepted by ICPP2022. Code is open-sourced!\n[2022.1] One paper for pushing the limit of post-training quantization has been accepted by ICLR2022.\n[2021] Our team wins the championship of LPCV 2021 FPGA Track and we also open source the solution.\n[2021] We release a model quantization benchmark MQBench and open source the model quantization toolkit.\n[2021] We release a model robustness benchmark RobustART and open source the evaluation framework.\n[2021] One paper accepted by CVPR2021 as oral presentation.\n[2021] Two papers accepted by ICCV2021.\n[2021] One paper accepted by ICLR2021. \n Download my resumé. --\r","date":1640995200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1657038936,"objectID":"dd1480a0826a21d038e33effc41a0f78","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"Ruihao Gong is currently a senior research manager and team leader for Efficient Model R\u0026D at SenseTime Research Model Toolchain Team under the supervision of Xiaogang Wang and Fengwei Yu. He focuses on the system and algorithms for accelerating the industry model production, model deployment and model efficiency.","tags":null,"title":"Ruihao Gong","type":"authors"},{"authors":[],"categories":null,"content":"","date":1653111000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1653111000,"objectID":"98c3cfb23d0ac319ed769db45fd343ab","permalink":"https://example.com/talk/when-industrial-model-toolchain-meets-xilinx-fpga/","publishdate":"2022-05-21T00:00:00Z","relpermalink":"/talk/when-industrial-model-toolchain-meets-xilinx-fpga/","section":"event","summary":"introduce the winner solution of LPCV 2021 FPGA track.","tags":[],"title":"When industrial model toolchain meets Xilinx FPGA","type":"event"},{"authors":["Liang Liu","Mingzhu Shen","Ruihao Gong","Fengwei Yu","Hailong Yang"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657038936,"objectID":"37c12c6dff396e765c8bb55eeaf777dd","permalink":"https://example.com/publication/2022-icpp-nnlqp/","publishdate":"2022-07-05T16:35:36.019791Z","relpermalink":"/publication/2022-icpp-nnlqp/","section":"publication","summary":"","tags":["neural network","multi-platform","latency query","latency prediction"],"title":"NNLQP: A Multi-Platform Neural Network Latency Query and Prediction System with An Evolving Database","type":"publication"},{"authors":["Xiuying Wei","Ruihao Gong","Yuhang Li","Xianglong Liu","Fengwei Yu"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648369342,"objectID":"3515857ad8bec5c61243a1f7a0facdc3","permalink":"https://example.com/publication/2022-iclr-qdrop/","publishdate":"2022-03-27T08:22:22.654761Z","relpermalink":"/publication/2022-iclr-qdrop/","section":"publication","summary":"","tags":[],"title":"QDrop: Randomly Dropping Quantization for Extremely Low-bit Post-Training Quantization","type":"publication"},{"authors":["Yuhang Li","Feng Zhu","Ruihao Gong","Mingzhu Shen","Xin Dong","Fengwei Yu","Shaoqing Lu","Shi Gu"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654758959,"objectID":"4441275e1fe57d1e622ba3cf1752c1b5","permalink":"https://example.com/publication/2021-iccv-mixmix/","publishdate":"2022-06-09T07:15:59.63863Z","relpermalink":"/publication/2021-iccv-mixmix/","section":"publication","summary":"","tags":[],"title":"MixMix: All You Need for Data-Free Compression Are Feature and Data Mixing","type":"publication"},{"authors":["Mingzhu Shen","Feng Liang","Ruihao Gong","Yuhang Li","Chuming Li","Chen Lin","Fengwei Yu","Junjie Yan","Wanli Ouyang"],"categories":[],"content":"","date":1633046400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654756516,"objectID":"bc2e6e8de478408f5140b43a1652bbc3","permalink":"https://example.com/publication/2021-iccv-oqa/","publishdate":"2022-06-09T06:34:53.78988Z","relpermalink":"/publication/2021-iccv-oqa/","section":"publication","summary":"","tags":[],"title":"Once Quantization-Aware Training: High Performance Extremely Low-Bit Architecture Search","type":"publication"},{"authors":[],"categories":null,"content":"A summary can be seen at Link.\n","date":1630090800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630090800,"objectID":"f7c436c4bf77b772d25113e40f45828b","permalink":"https://example.com/talk/research-on-post-training-quantization-from-classic-to-original/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/talk/research-on-post-training-quantization-from-classic-to-original/","section":"event","summary":"Our research journal on PTQ for a better industry application.","tags":[],"title":"Research on Post-training Quantization - From classic to original","type":"event"},{"authors":null,"categories":null,"content":"This project contains the complete implementation that wins the LPCV 2021 FPGA track, which can run an object detection model with an extremely high efficiency and accuracy.\nThe leaderboard can be seen at LPCV2021 Leaderboard (Spring Team).\n","date":1630022400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630022400,"objectID":"4c4f90bd2047d38d22bbc2376fc38051","permalink":"https://example.com/project/lpcv2021-fpga-track/","publishdate":"2021-08-27T00:00:00Z","relpermalink":"/project/lpcv2021-fpga-track/","section":"project","summary":"The implementaion of winner solution for LPCV 2021 FPGA track","tags":["Deep Learning"],"title":"LPCV2021 Winner Solution of FPGA Track","type":"project"},{"authors":["Ruihao Gong"],"categories":["Deep learning compiler","Sparsity"],"content":"For details, please refer to Link.\n","date":1629936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629936000,"objectID":"ba8d27cf66c13e30779619ef8bc2a7d5","permalink":"https://example.com/post/2021-tvm-sparsity/","publishdate":"2021-08-26T00:00:00Z","relpermalink":"/post/2021-tvm-sparsity/","section":"post","summary":"Realize efficient sparsity with 50% sparsity by TVM.","tags":["Sparsity","TVM"],"title":"Block Sparsity on TVM -- 50% spasity = 1.6 speed up + \u003c 1% accuracy loss","type":"post"},{"authors":["Ruihao Gong"],"categories":["Quantization","Framework"],"content":"For details, please refer to Link.\n","date":1629158400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1629158400,"objectID":"f5a21c2ca7142c3406f60a5f58759a61","permalink":"https://example.com/post/2021-quant-tool/","publishdate":"2021-08-17T00:00:00Z","relpermalink":"/post/2021-quant-tool/","section":"post","summary":"Introduce some quantization frameworks from hardware vendors or universities.","tags":["Quantization"],"title":"Quantization frameworks","type":"post"},{"authors":null,"categories":null,"content":"MQBench is an open-source model quantization toolkit based on PyTorch fx.\nThe envision of MQBench is to provide:\n SOTA Algorithms. With MQBench, the hardware vendors and researchers can benefit from the latest research progress in academia. Powerful Toolkits. With the toolkit, quantization node can be inserted to the original PyTorch module automatically with respect to the specific hardware. After training, the quantized model can be smoothly converted to the format that can inference on the real device.  The documentation can be seen at Documentation and the website is at Link.\n","date":1627344000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627344000,"objectID":"fe1a5b080e05f4456bd0c49e563c4c9f","permalink":"https://example.com/project/mqbench/","publishdate":"2021-07-27T00:00:00Z","relpermalink":"/project/mqbench/","section":"project","summary":"MQBench is an open-source model quantization toolkit based on PyTorch fx, which provides **SOTA Algorithms** and **Powerful Toolkits**.","tags":["Deep Learning"],"title":"MQBench","type":"project"},{"authors":["Yuhang Li","Mingzhu Shen","Jian Ma","Yan Ren","Mingxin Zhao","Qi Zhang","Ruihao Gong","Fengwei Yu","Junjie Yan"],"categories":[],"content":"","date":1625097600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648369334,"objectID":"8bf3720e1e2a1e21675a4f06a182fb6d","permalink":"https://example.com/publication/2021-neuips-mqbench/","publishdate":"2022-03-27T08:22:14.729101Z","relpermalink":"/publication/2021-neuips-mqbench/","section":"publication","summary":"","tags":[],"title":"MQBench: Towards Reproducible and Deployable Model Quantization Benchmark","type":"publication"},{"authors":null,"categories":null,"content":"RobustART is a comprehensive Robustness investigation benchmark on ImageNet regarding ARchitectural design (49 human-designed off-the-shelf architectures and neural architecture searched networks) and Training techniques (10+ general ones e.g., extra training data, etc) towards diverse noises (adversarial, natural, and system noises).\nThe benchmark (including open-source toolkit, pre-trained model zoo, datasets, and analyses):\n presents an open-source platform for conducting comprehensive evaluation on diverse robustness types; provides a variety of pre-trained models with different training techniques to facilitate robustness evaluation; proposes a new view to better understand the mechanism towards designing robust DNN architectures, backed up by the analysis.  For more details, please refer to the website. We will continuously contribute to building this ecosystem for the community.\n","date":1624752000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1624752000,"objectID":"3dd9814f55d90c7117890dfb89abfe54","permalink":"https://example.com/project/robustart/","publishdate":"2021-06-27T00:00:00Z","relpermalink":"/project/robustart/","section":"project","summary":"RobustART is a comprehensive Robustness investigation benchmark on ImageNet regarding ARchitectural design and Training techniques.","tags":["Deep Learning"],"title":"RobustART","type":"project"},{"authors":["Xiangguo Zhang","Haotong Qin","Yifu Ding","Ruihao Gong","Qinghua Yan","Renshuai Tao","Yuhang Li","Fengwei Yu","Xianglong Liu"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366064,"objectID":"7603f1b906e99980ef3c1d514213e767","permalink":"https://example.com/publication/2021-cvpr-dsg/","publishdate":"2022-03-27T07:29:01.151846Z","relpermalink":"/publication/2021-cvpr-dsg/","section":"publication","summary":"","tags":[],"title":"Diversifying Sample Generation for Accurate Data-Free Quantization","type":"publication"},{"authors":[],"categories":null,"content":"","date":1620327600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620327600,"objectID":"853f96f86b48b0bf7142c165300af0e8","permalink":"https://example.com/talk/neural-network-model-compression/","publishdate":"2021-05-06T00:00:00Z","relpermalink":"/talk/neural-network-model-compression/","section":"event","summary":"A course about neural network compression for undergraduates and graduates of Tsinghua University.","tags":[],"title":"Neural Network Model Compression","type":"event"},{"authors":["Ruihao Gong"],"categories":["Deep learning compiler","Quantization","Transformer"],"content":"For details, please refer to Link.\n","date":1618790400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618790400,"objectID":"3a1bd9f77a5faf31d5e4b793cbef8317","permalink":"https://example.com/post/2021-vit-tvm-quant/","publishdate":"2021-04-19T00:00:00Z","relpermalink":"/post/2021-vit-tvm-quant/","section":"post","summary":"The first to support Int8 ViT for TVM, achieving a significant speed up.","tags":["Transformer","Quantization"],"title":"Int8 ViT on TVM, 1.5 speed up compared with TensorRT","type":"post"},{"authors":[],"categories":null,"content":"A summary can be seen at Link.\n","date":1618426800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1618426800,"objectID":"5bcd7575cd4c816404d85f66bcdf08be","permalink":"https://example.com/talk/an-introduction-to-block-reconstruction-quantization/","publishdate":"2021-04-14T00:00:00Z","relpermalink":"/talk/an-introduction-to-block-reconstruction-quantization/","section":"event","summary":"An introduction to a SOTA PTQ algorithm - BRECQ.","tags":[],"title":"An Introduction to Block Reconstruction Quantization","type":"event"},{"authors":["Ruihao Gong"],"categories":["Quantization"],"content":"For details, please refer to Link.\n","date":1612742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1612742400,"objectID":"04a5137a00e40e6ac6826e565ef89410","permalink":"https://example.com/post/2021-quant-gap/","publishdate":"2021-02-08T00:00:00Z","relpermalink":"/post/2021-quant-gap/","section":"post","summary":"Analyze the gap between the academia and instustry.","tags":["Quantization"],"title":"Gap bettween the academia and industry, from the quantiztion perspective","type":"post"},{"authors":["Yuhang Li","Ruihao Gong","Xu Tan","Yang Yang","Peng Hu","Qi Zhang","Fengwei Yu","Wei Wang","Shi Gu"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366065,"objectID":"107942126dbe50a9ab43652f0eccd489","permalink":"https://example.com/publication/2021-iclr-brecq/","publishdate":"2022-03-27T07:29:01.723129Z","relpermalink":"/publication/2021-iclr-brecq/","section":"publication","summary":"","tags":[],"title":"BRECQ: Pushing the Limit of Post-Training Quantization by Block Reconstruction","type":"publication"},{"authors":["Shiyu Tang","Ruihao Gong","Yan Wang","Aishan Liu","Jiakai Wang","Xinyun Chen","Fengwei Yu","Xianglong Liu","Dawn Song","Alan Yuille","Philip H. S. Torr","Dacheng Tao"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648369338,"objectID":"b6a3e2e944c7b43fff70bb715f7c1187","permalink":"https://example.com/publication/2021-arxiv-robustart/","publishdate":"2022-03-27T08:22:18.567278Z","relpermalink":"/publication/2021-arxiv-robustart/","section":"publication","summary":"","tags":[],"title":"RobustART: Benchmarking Robustness on Architecture Design and Training Techniques","type":"publication"},{"authors":[],"categories":null,"content":"A summary can be seen at Link\n","date":1591729200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591729200,"objectID":"dbfd7701d8642bf2d375412f65ad088e","permalink":"https://example.com/talk/an-introduction-to-quantization/","publishdate":"2020-06-09T00:00:00Z","relpermalink":"/talk/an-introduction-to-quantization/","section":"event","summary":"An introduction to the basics of model quantization and the relavant effective algorithms.","tags":[],"title":"An Introduction to Quantization","type":"event"},{"authors":["Haotong Qin","Ruihao Gong","Xianglong Liu","Mingzhu Shen","Ziran Wei","Fengwei Yu","Jingkuan Song"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366067,"objectID":"b9430da794dbd1743eccf5e9deb72237","permalink":"https://example.com/publication/2020-cvpr-irnet/","publishdate":"2022-03-27T07:29:03.373611Z","relpermalink":"/publication/2020-cvpr-irnet/","section":"publication","summary":"","tags":[],"title":"Forward and Backward Information Retention for Accurate Binary Neural Networks","type":"publication"},{"authors":["Yudong Wu","Yichao Wu","Ruihao Gong","Yuanhao Lv","Ken Chen","Ding Liang","Xiaolin Hu","Xianglong Liu","Junjie Yan"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366067,"objectID":"4f56f2df3811eb5cfff12a63bac70e85","permalink":"https://example.com/publication/2020-cvpr-rcmloss/","publishdate":"2022-03-27T07:29:03.922862Z","relpermalink":"/publication/2020-cvpr-rcmloss/","section":"publication","summary":"","tags":[],"title":"Rotation Consistent Margin Loss for Efficient Low-bit Face Recognition","type":"publication"},{"authors":["Feng Zhu","Ruihao Gong","Fengwei Yu","Xianglong Liu","Yanfei Wang","Zhelong Li","Xiuqi Yang","Junjie Yan"],"categories":[],"content":"","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366068,"objectID":"ad1126ec12d50f3ae21276375e7b9b69","permalink":"https://example.com/publication/2020-cvpr-int-8-training/","publishdate":"2022-03-27T07:29:04.472601Z","relpermalink":"/publication/2020-cvpr-int-8-training/","section":"publication","summary":"","tags":[],"title":"Towards Unified INT8 Training for Convolutional Neural Network","type":"publication"},{"authors":["Mingzhu Shen","Xianglong Liu","Ruihao Gong","Kai Han"],"categories":[],"content":"","date":1588291200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366065,"objectID":"0aebb1b84ac5e9a160c6f974f10a1c5e","permalink":"https://example.com/publication/2020-icassp-bbg/","publishdate":"2022-03-27T07:29:02.282811Z","relpermalink":"/publication/2020-icassp-bbg/","section":"publication","summary":"","tags":[],"title":"Balanced Binary Neural Networks with Gated Residual","type":"publication"},{"authors":["Yuhang Li","Ruihao Gong","Fengwei Yu","Xin Dong","Xianglong Liu"],"categories":[],"content":"","date":1585699200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366066,"objectID":"4dbd646e758b7ce6813e79376e788921","permalink":"https://example.com/publication/2020-iclrw-dms/","publishdate":"2022-03-27T07:29:02.826575Z","relpermalink":"/publication/2020-iclrw-dms/","section":"publication","summary":"","tags":[],"title":"DMS: Differentiable Dimension Search for Binary Neural Networks","type":"publication"},{"authors":["Haotong Qin","Ruihao Gong","Xianglong Liu","Xiao Bai","Jingkuan Song","Nicu Sebe"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366068,"objectID":"212f185d09ce3c066ebedcc2e396d21a","permalink":"https://example.com/publication/2020-pr-bnnsurvey/","publishdate":"2022-03-27T07:29:05.018687Z","relpermalink":"/publication/2020-pr-bnnsurvey/","section":"publication","summary":"The binary neural network, largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error, improving the network loss function, and reducing the gradient error. We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification, object detection and semantic segmentation. Finally, the challenges that may be faced in future research are prospected.","tags":["Binary neural network","Deep learning","Model compression","Network quantization","Model acceleration"],"title":"Binary neural networks: A survey","type":"publication"},{"authors":["Yuhang Li","Wei Wang","Haoli Bai","Ruihao Gong","Xin Dong","Fengwei Yu"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366062,"objectID":"32738930d483fa6f4e26a6a2115188b4","permalink":"https://example.com/publication/2020-arxiv-ebs/","publishdate":"2022-03-27T07:28:59.476187Z","relpermalink":"/publication/2020-arxiv-ebs/","section":"publication","summary":"","tags":[],"title":"Efficient Bitwidth Search for Practical Mixed Precision Neural Network","type":"publication"},{"authors":["Qingchang Han","Yongmin Hu","Fengwei Yu","Hailong Yang","Bing Liu","Peng Hu","Ruihao Gong","Yanfei Wang","Rui Wang","Zhongzhi Luan","Depei Qian"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366064,"objectID":"5b58a06350e087633976687b4a9bdca9","permalink":"https://example.com/publication/2020-icpp-lowbit/","publishdate":"2022-03-27T07:29:00.583344Z","relpermalink":"/publication/2020-icpp-lowbit/","section":"publication","summary":"With the continuous demand for higher accuracy of deep neural networks, the model size has increased significantly. Quantization is one of the most widely used model compression methods, which can effectively reduce the model size without severe accuracy loss. Modern processors such as ARM CPU and NVIDIA GPU have already provided the support of low-bit arithmetic instructions. However, there lack efficient and practical optimizations for convolution computation towards extremely low-bit on ARM CPU (e.g., 2 ∼ 8-bit) and NVIDIA GPU (e.g., 4-bit and 8-bit). This paper explores the performance optimization methods of extremely low-bit convolution on diverse architectures. On ARM CPU, we propose two instruction schemes for 2 ∼ 3-bit and 4 ∼ 8-bit convolution with corresponding register allocation methods. In addition, we re-design the GEMM computation with data padding and packing optimizations. We also implement winograd algorithm for convolution with some specific bit width (e.g., 4 ∼ 6-bit) to achieve higher performance. On NVIDIA GPU, we propose a data partition mechanism and multi-level memory access optimizations, to better adapt the computation to GPU thread and memory hierarchy. We also propose quantization fusion to eliminate unnecessary data access. The experiment results demonstrate our implementations achieve better performance of extremely low-bit convolution compared to the state-of-the-art frameworks and libraries such as ncnn and cuDNN. To the best of our knowledge, this is the first work that provides efficient implementations of extremely low-bit convolutions covering 2 ∼ 8-bit on ARM CPU and 4-bit/8-bit on NVIDIA GPU. ","tags":["NVIDIA GPU","Quantized Neural Network","Extremely Low-bit Convolution","Computation Optimization","ARM CPU"],"title":"Extremely Low-Bit Convolution Optimization for Quantized Neural Network on Modern Computer Architectures","type":"publication"},{"authors":["Ruihao Gong","Xianglong Liu","Shenghu Jiang","Tianxiang Li","Peng Hu","Jiazhen Lin","Fengwei Yu","Junjie Yan"],"categories":[],"content":"","date":1569888000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1648366063,"objectID":"b2b8d8079ac76ff65a1d3a5926745606","permalink":"https://example.com/publication/2019-iccv-dsq/","publishdate":"2022-03-27T07:29:00.032371Z","relpermalink":"/publication/2019-iccv-dsq/","section":"publication","summary":"","tags":[],"title":"Differentiable Soft Quantization: Bridging Full-Precision and Low-Bit Neural Networks","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34;\rif porridge == \u0026#34;blueberry\u0026#34;:\rprint(\u0026#34;Eating...\u0026#34;)\r  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}}\r{{% fragment %}} **Two** {{% /fragment %}}\r{{% fragment %}} Three {{% /fragment %}}\r Press Space to play!\nOne \rTwo \rThree \r A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}}\r- Only the speaker can read these notes\r- Press `S` key to view\r{{% /speaker_note %}}\r Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}}\r{{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}}\r{{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}\r  Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1,\r.reveal section h2,\r.reveal section h3 {\rcolor: navy;\r}\r  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"}]